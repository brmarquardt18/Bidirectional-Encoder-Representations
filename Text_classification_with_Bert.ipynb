{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text classification with Bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERvxWGIcGPOq"
      },
      "source": [
        "In this notebook I will be going over a text clasification with a BERT tokenizer model using much of the code from https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/. I had done text classification of the same dataset in an earlier notebook so I am curious to see if using BERT will improve the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2yhJx8a-xYf",
        "outputId": "10b05dfb-477f-4510-d8e9-8211c6d96528"
      },
      "source": [
        "!pip install bert-for-tf2\r\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP1aVei9SQ44"
      },
      "source": [
        "I am running this in Google Colab which by default will not run the script of TensorFlow 2.0, meaning that we need to add some lines to make this happen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6eW7oZd-2xS"
      },
      "source": [
        "try:\r\n",
        "    %tensorflow_version 2.x\r\n",
        "except Exception:\r\n",
        "    pass\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "#library with a bunch of pretrained models developed in TensorFlow\r\n",
        "import tensorflow_hub as hub\r\n",
        "\r\n",
        "from tensorflow.keras import layers\r\n",
        "import bert\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjEkLp62RqQa"
      },
      "source": [
        "The dataset is a standard movie review and corresponding sentiment (positive or negative). The data set contains 50,000 reviews and sentiment pairs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgvDLn4T-66j",
        "outputId": "3bd74920-b96f-4007-fd21-2d159d119f12"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "movie_reviews = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/IMDB Dataset.csv\")\r\n",
        "\r\n",
        "movie_reviews.isnull().values.any()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWbW2iwsSepb"
      },
      "source": [
        "## Preprocessing\r\n",
        "The following chunk will preprocess the data by removing punctuation and special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIjMcvSR_e_y"
      },
      "source": [
        "def preprocess_text(sen):\r\n",
        "    # Removing html tags\r\n",
        "    sentence = remove_tags(sen)\r\n",
        "\r\n",
        "    # Remove punctuations and numbers\r\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\r\n",
        "\r\n",
        "    # Single character removal\r\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\r\n",
        "\r\n",
        "    # Removing multiple spaces\r\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\r\n",
        "\r\n",
        "    return sentence"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CYfec0Z_8uI"
      },
      "source": [
        "import re\r\n",
        "#Return the cleaned text\r\n",
        "TAG_RE = re.compile(r'<[^>]+>')\r\n",
        "\r\n",
        "def remove_tags(text):\r\n",
        "    return TAG_RE.sub('', text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0XcqBrJ_-uG"
      },
      "source": [
        "reviews = []\r\n",
        "sentences = list(movie_reviews['review'])\r\n",
        "for sen in sentences:\r\n",
        "    reviews.append(preprocess_text(sen))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv1AKZ00TXsR"
      },
      "source": [
        "Now that the data is cleaned, I will just show the format that the data is in. There are two columns, one for the review and the other for the sentiment as shown by the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU_xad7wAG3_",
        "outputId": "38db0fe6-6302-446b-ef41-403b8a8b6ae5"
      },
      "source": [
        "print(movie_reviews.columns.values)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['review' 'sentiment']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh2wi4kgTtgi"
      },
      "source": [
        "The following cell will show you the format of the sentiment as either \"positive\" or \"negative\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjYFnBGTAI2c",
        "outputId": "0921520e-570c-45cc-ab73-e4d434ccdb25"
      },
      "source": [
        "movie_reviews.sentiment.unique()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'negative'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwe0_SJET8Ww"
      },
      "source": [
        "There is no reason to leave the sentiment in the string of \"positive\" or \"negative\" so we convert that into 1 for positive and 0 for negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZoV7rYcALIE"
      },
      "source": [
        "y = movie_reviews['sentiment']\r\n",
        "\r\n",
        "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liACHx64T7Cr"
      },
      "source": [
        "Now we have two different arrays, reviews and y which contain the reviews and the sentiment respectively. To see the format of the data now the following cells will show the first data point for each array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ABrk7T5AM0r",
        "outputId": "5b1816e8-2edc-458d-d3e1-fe7c40840b0e"
      },
      "source": [
        "print(reviews[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One of the other reviewers has mentioned that after watching just Oz episode you ll be hooked They are right as this is exactly what happened with me The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the word It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to many Aryans Muslims gangstas Latinos Christians Italians Irish and more so scuffles death stares dodgy dealings and shady agreements are never far away would say the main appeal of the show is due to the fact that it goes where other shows wouldn dare Forget pretty pictures painted for mainstream audiences forget charm forget romance OZ doesn mess around The first episode ever saw struck me as so nasty it was surreal couldn say was ready for it but as watched more developed taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards who ll be sold out for nickel inmates who ll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ5i6w_8AR2I",
        "outputId": "fc806bef-7fb0-4f76-d1cd-7f3318ca977a"
      },
      "source": [
        "print(y[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrgyUKbpUpe3"
      },
      "source": [
        "## Tokenization\r\n",
        "We cannot use the BERT embeddings as an input if we do not tokenize the reviews first. The following cell will provide the BERT tokenization. The url inputted as a string contains a saved model in the form of a TensorFlow 2 model. This is a BERT model from the TensorFlow models repository on GitHub at https://github.com/tensorflow/models/tree/master/official/nlp/bert. The model has 12 hidden layers, a hidden size of 768, and 12 attention heads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKcA9uyZATwW"
      },
      "source": [
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\r\n",
        "#create a BERT embedding layer by importing the BERT model from hub.KerasLayer\r\n",
        "#trainable parameter is false since we will not further train the model\r\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\r\n",
        "                            trainable=False)\r\n",
        "#create a BERT vocabulary file\r\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
        "#set the text to lowercase\r\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
        "#pass vocabulary_file and to_lower_case to the BertTokenizer object\r\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1P_nKJDXFeK"
      },
      "source": [
        "To show the form of the tonization of a sentence I created a random sentence to pass in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42b67SPtAV8E",
        "outputId": "f2c81977-270e-4a85-faa2-889760f4dad2"
      },
      "source": [
        "tokenizer.tokenize(\"I don't think that you're able to visit Henry's house.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'don',\n",
              " \"'\",\n",
              " 't',\n",
              " 'think',\n",
              " 'that',\n",
              " 'you',\n",
              " \"'\",\n",
              " 're',\n",
              " 'able',\n",
              " 'to',\n",
              " 'visit',\n",
              " 'henry',\n",
              " \"'\",\n",
              " 's',\n",
              " 'house',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJPtFqJwAeI2",
        "outputId": "a5d0dedf-9609-45e0-aa10-6b581341c388"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"I don't think that you're able to visit Henry's house.\"))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1045,\n",
              " 2123,\n",
              " 1005,\n",
              " 1056,\n",
              " 2228,\n",
              " 2008,\n",
              " 2017,\n",
              " 1005,\n",
              " 2128,\n",
              " 2583,\n",
              " 2000,\n",
              " 3942,\n",
              " 2888,\n",
              " 1005,\n",
              " 1055,\n",
              " 2160,\n",
              " 1012]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ox1WTOX5Xz"
      },
      "source": [
        "The following cell contains a simple function that will return the id numbers of the tokenized words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7XnJI0oAgA7"
      },
      "source": [
        "def tokenize_reviews(text_reviews):\r\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulSnpcQkYJGc"
      },
      "source": [
        "Now to actually tokenize every review in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moPGUCRYAkMb"
      },
      "source": [
        "tokenized_reviews = [tokenize_reviews(review) for review in reviews]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqZCMCMbelvl"
      },
      "source": [
        "## Preparing to Train\r\n",
        "Since each review can be a different length from the other ones, we must figure out how to fix this. A potentials solution would be to pad the sentences by 0s. However this can lead to the matrix being much less filled for the smaller sentences. To mitigate this, we will pad for each batch. So while there will still be some sparse matrices, we will only have to pad to the length of the largest sentence in each batch.\r\n",
        "\r\n",
        "To help us do this, we will have to find the length of each sentence. The following cell will create an array containing each review, the sentiment, and the length. After that we will shuffle the data because the current form has positive and negative reviews separated. After that we will sort the data by the length of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rqijZS4Al3_"
      },
      "source": [
        "reviews_with_len = [[review, y[i], len(review)]\r\n",
        "                 for i, review in enumerate(tokenized_reviews)]\r\n",
        "import random\r\n",
        "random.shuffle(reviews_with_len)\r\n",
        "reviews_with_len.sort(key=lambda x: x[2])\r\n",
        "# remove length since it is not needed anymore\r\n",
        "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "astDI2gchbim"
      },
      "source": [
        "The following cell will convert the data into a TensorFlow 2.0 compliant input dataset shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyoP8gmHAsNC"
      },
      "source": [
        "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dsj4VXaJiHTW"
      },
      "source": [
        "The following cell is where the padding will occur. Using a batch size of 32, we will pad the reviews locally by batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX-XIxciAthX"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZIiRuXyijNE"
      },
      "source": [
        "The following cell will show how the padding is applied. Notice the 0s chained on at the end of each array in the beginning but the arrays at the end do not need padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0HcSr3ZBQsL",
        "outputId": "1f962301-a268-4250-b63e-2b0371f032e9"
      },
      "source": [
        "next(iter(batched_dataset))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(32, 21), dtype=int32, numpy=\n",
              " array([[ 3191,  1996,  2338,  5293,  1996,  3185,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2054,  5896,  2054,  2466,  2054,  6752,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 3078,  5436,  3078,  3257,  3532,  7613,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2062, 23873,  3993,  2062, 11259,  2172,  2172,  2062, 14888,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 1045,  2876,  9278,  2023,  2028,  2130,  2006,  7922, 12635,\n",
              "          2305,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2023,  3185,  2003,  6659,  2021,  2009,  2038,  2070,  2204,\n",
              "          3896,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 1045,  3246,  2023,  2177,  1997,  2143, 11153,  2196,  2128,\n",
              "         15908,  2015,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 8235,  1998,  3048,  4616,  2011,  3419,  2457, 27727,  1998,\n",
              "          2848, 16133,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 7918, 14674,  7662,  2003,  6581,  2003,  2023,  2143,  2002,\n",
              "          3084, 17160,  2450,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [11861,  1996, 21442,  6895,  3238,  2515,  2210, 22759,  6198,\n",
              "          1998,  3185,  2087, 12487,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2023,  2003,  2307,  3185,  2205,  2919,  2009,  2003,  2025,\n",
              "          2800,  2006,  2188,  2678,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2017,  2488,  5454,  2703,  2310, 25032,  8913,  8159,  2130,\n",
              "          2065,  2017,  2031,  3427,  2009,     0,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2053,  7615,  5236,  3185,  3772,  2779,  2030,  4788,  9000,\n",
              "          2053,  3168,  2012,  2035, 13558,  2009,     0,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 1045,  2123,  2113,  2339,  2066,  2023,  3185,  2061,  2092,\n",
              "          2021,  2196,  2131,  5458,  1997,  3666,  2009,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2146, 11771,  1038,  8523,  8458,  6633,  3560,  2196,  2031,\n",
              "          2042,  2061,  5580,  2000,  2156,  4566,  6495,  4897,     0,\n",
              "             0,     0,     0],\n",
              "        [ 7615,  2023,  3185,  2003,  5263,  2003,  6659,  2200, 17727,\n",
              "          3217,  3676,  3468,  2919,  7613,  3257,  2025,  2298,     0,\n",
              "             0,     0,     0],\n",
              "        [ 2074,  2293,  1996,  6970, 13068,  2090,  2048,  2307,  3494,\n",
              "          1997,  2754,  3898,  2310,  3593,  2102,  6287,  5974,     0,\n",
              "             0,     0,     0],\n",
              "        [ 7078, 10392,  3649,  2360,  2876,  2079,  2023,  2104,  9250,\n",
              "          3185,  1996,  3425,  2009, 17210,  3422,  2009,  2085, 10392,\n",
              "             0,     0,     0],\n",
              "        [ 5587,  2023,  2210, 17070,  2000,  2115,  2862,  1997,  6209,\n",
              "         24945,  2009, 26354, 28394,  2102,  6057,  1998,  2203, 27242,\n",
              "             0,     0,     0],\n",
              "        [ 2023,  2003,  2204,  2143,  2023,  2003,  2200,  6057,  2664,\n",
              "          2044,  2023,  2143,  2045,  2020,  2053,  2204,  8471,  3152,\n",
              "             0,     0,     0],\n",
              "        [ 2023,  2003,  1996, 15764,  3185,  2544,  1997,  8429, 24905,\n",
              "         17988,  7659,  2498,  2021,  2045,  2024,  2053, 13842,  5312,\n",
              "             0,     0,     0],\n",
              "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
              "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
              "             0,     0,     0],\n",
              "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
              "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
              "             0,     0,     0],\n",
              "        [ 1037,  7244,  3185,  2009,  2003,  2440,  1997,  6699,  1998,\n",
              "          6919,  3772,  2071,  2031,  2938,  2083,  2009,  2117,  2051,\n",
              "             0,     0,     0],\n",
              "        [ 1037,  5790,  1997,  2515,  2025,  4088,  2000,  4671,  2129,\n",
              "         10634,  2139, 24128,  1998, 21660,  2135,  2919,  2023,  3185,\n",
              "          2003,     0,     0],\n",
              "        [ 1037,  2033,  6491, 11124,  6774,  2143,  2008,  5121,  7906,\n",
              "          2115,  3086,  3841, 13196,  2003, 17160,  1998, 26103,  2000,\n",
              "          3422,     0,     0],\n",
              "        [ 6283,  2009,  2007,  2035,  2026,  2108,  5409,  3185,  2412,\n",
              "         10597, 21985,  2393,  2033,  2009,  2001,  2008,  2919,  3404,\n",
              "          2033,     0,     0],\n",
              "        [ 2005,  5760,  7788,  4393,  8808,  2498,  2064, 12826,  2000,\n",
              "          1996, 11056,  3152,  3811, 16755,  2169,  1998,  2296,  2028,\n",
              "          1997,  2068,     0],\n",
              "        [ 7244,  2092,  2856, 10828,  1997, 10904,  2402,  2472,  3135,\n",
              "          2293,  2466,  2007, 10958,  8428, 10102,  1999,  1996,  4281,\n",
              "          4276,  3773,     0],\n",
              "        [ 2028,  1997,  1996,  4569, 15580,  2102,  5691,  2081,  1999,\n",
              "          3522,  2086,  2204, 23191,  5436,  1998, 11813,  6370,  2191,\n",
              "          2023,  2028,  4438],\n",
              "        [ 2023,  2003,  6659,  3185,  2123,  5949,  2115,  2769,  2006,\n",
              "          2009,  2123,  2130,  3422,  2009,  2005,  2489,  2008,  2035,\n",
              "          2031,  2000,  2360],\n",
              "        [ 2307,  3185,  2926,  1996,  2189,  3802,  2696,  2508,  2012,\n",
              "          2197,  2023,  8847,  6702,  2043,  2017,  2031,  2633,  2179,\n",
              "          2008,  2569,  2619]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 0, 1, 1, 1, 0, 0], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2pQl33vuUJP"
      },
      "source": [
        "The following cell will separate the data into train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5GiaslyBSWB"
      },
      "source": [
        "import math\r\n",
        "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\r\n",
        "TEST_BATCHES = TOTAL_BATCHES // 10\r\n",
        "batched_dataset.shuffle(TOTAL_BATCHES)\r\n",
        "test_data = batched_dataset.take(TEST_BATCHES)\r\n",
        "train_data = batched_dataset.skip(TEST_BATCHES)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW1GgK6yvFgb"
      },
      "source": [
        "## Creating Model\r\n",
        "Now we are all set to create our model. To do so, we will create a class that inherits from the tf.keras.Model class. Inside the class we will define our model layers which will consist of three convolutional neural network layers with a glabal max pooling. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikvP7Y6rBYSU"
      },
      "source": [
        "class TEXT_MODEL(tf.keras.Model):\r\n",
        "    \r\n",
        "    def __init__(self,\r\n",
        "                 vocabulary_size,\r\n",
        "                 embedding_dimensions=128,\r\n",
        "                 cnn_filters=50,\r\n",
        "                 dnn_units=512,\r\n",
        "                 model_output_classes=2,\r\n",
        "                 dropout_rate=0.1,\r\n",
        "                 training=False,\r\n",
        "                 name=\"text_model\"):\r\n",
        "        super(TEXT_MODEL, self).__init__(name=name)\r\n",
        "        \r\n",
        "        self.embedding = layers.Embedding(vocabulary_size,\r\n",
        "                                          embedding_dimensions)\r\n",
        "        #three convolutional neural network layers\r\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\r\n",
        "                                        kernel_size=2,\r\n",
        "                                        padding=\"valid\",\r\n",
        "                                        activation=\"relu\")\r\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\r\n",
        "                                        kernel_size=3,\r\n",
        "                                        padding=\"valid\",\r\n",
        "                                        activation=\"relu\")\r\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\r\n",
        "                                        kernel_size=4,\r\n",
        "                                        padding=\"valid\",\r\n",
        "                                        activation=\"relu\")\r\n",
        "        #global max pooling is applied to the output of each of the convolutional neural network layer\r\n",
        "        self.pool = layers.GlobalMaxPool1D()\r\n",
        "        \r\n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\r\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\r\n",
        "        if model_output_classes == 2:\r\n",
        "            self.last_dense = layers.Dense(units=1,\r\n",
        "                                           activation=\"sigmoid\")\r\n",
        "        else:\r\n",
        "            self.last_dense = layers.Dense(units=model_output_classes,\r\n",
        "                                           activation=\"softmax\")\r\n",
        "    \r\n",
        "    def call(self, inputs, training):\r\n",
        "        l = self.embedding(inputs)\r\n",
        "        l_1 = self.cnn_layer1(l) \r\n",
        "        l_1 = self.pool(l_1) \r\n",
        "        l_2 = self.cnn_layer2(l) \r\n",
        "        l_2 = self.pool(l_2)\r\n",
        "        l_3 = self.cnn_layer3(l)\r\n",
        "        l_3 = self.pool(l_3) \r\n",
        "        #three convolutional neural network layers are concatenated together and their output is fed to the first densely connected neural network\r\n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\r\n",
        "        concatenated = self.dense_1(concatenated)\r\n",
        "        concatenated = self.dropout(concatenated, training)\r\n",
        "        model_output = self.last_dense(concatenated)\r\n",
        "        \r\n",
        "        return model_output"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rqP7nULw4Cz"
      },
      "source": [
        "The following cell contains the values we will use for our hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NDGvJ0KBcO7"
      },
      "source": [
        "VOCAB_LENGTH = len(tokenizer.vocab)\r\n",
        "EMB_DIM = 200\r\n",
        "CNN_FILTERS = 100\r\n",
        "DNN_UNITS = 256\r\n",
        "OUTPUT_CLASSES = 2\r\n",
        "\r\n",
        "DROPOUT_RATE = 0.2\r\n",
        "\r\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mBE1909w_2Y"
      },
      "source": [
        "In the next cell we will create an object of the class and pass the hyper parameters in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJySUZjsBd9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ad574e-3124-4fab-ded8-dc3ed9c5175c"
      },
      "source": [
        "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\r\n",
        "                        embedding_dimensions=EMB_DIM,\r\n",
        "                        cnn_filters=CNN_FILTERS,\r\n",
        "                        dnn_units=DNN_UNITS,\r\n",
        "                        model_output_classes=OUTPUT_CLASSES,\r\n",
        "                        dropout_rate=DROPOUT_RATE)\r\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOcmMm2VzrMz"
      },
      "source": [
        "Before we can actually train the models we need to compile it. The following script compiles the model. Since there are only two output classes we can use the binary crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNJq3nP4Bfj0"
      },
      "source": [
        "\r\n",
        "text_model.compile(loss=\"binary_crossentropy\",\r\n",
        "                       optimizer=\"adam\",\r\n",
        "                       metrics=[\"accuracy\"])\r\n",
        "\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEJt2gylzuWb"
      },
      "source": [
        "Now we can actually train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkVQ9hhlBhKX",
        "outputId": "4c2612b7-87d3-420b-f950-dd167f12b48f"
      },
      "source": [
        "text_model.fit(train_data, epochs=NB_EPOCHS)\r\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1407/1407 [==============================] - 140s 98ms/step - loss: 0.3864 - accuracy: 0.8068\n",
            "Epoch 2/5\n",
            "1407/1407 [==============================] - 139s 98ms/step - loss: 0.1626 - accuracy: 0.9398\n",
            "Epoch 3/5\n",
            "1407/1407 [==============================] - 139s 98ms/step - loss: 0.0747 - accuracy: 0.9751\n",
            "Epoch 4/5\n",
            "1407/1407 [==============================] - 140s 99ms/step - loss: 0.0331 - accuracy: 0.9886\n",
            "Epoch 5/5\n",
            "1407/1407 [==============================] - 140s 99ms/step - loss: 0.0198 - accuracy: 0.9925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1433918e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLChc8POBirw",
        "outputId": "089814f8-7abe-4f92-dee8-094237fe6d54"
      },
      "source": [
        "results = text_model.evaluate(test_data)\r\n",
        "print(results)\r\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 17s 106ms/step - loss: 0.6821 - accuracy: 0.8826\n",
            "[0.6821133494377136, 0.8826121687889099]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQPVSkhvz02K"
      },
      "source": [
        "## Final Thoughts\r\n",
        "In this notebook I showed how to use the BERT tokenizer to create word embeddings that can be used to perform text classification. For the class with the 3 CNNs, we got an accuracy of .8826. I did a similar classification of movie reviews with the same dataset in an earlier notebook and was not able to get an accuracy above 88%. So it seems like this may be a better method for this type of classification but it is not abundantly clear because the results were about the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFv715HGQdvu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}